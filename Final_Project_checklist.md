# Final Project Checklist

**Чеклист для финальных проектов по основам ML&DL.**

Финальный проект объединяет в себе два соревнования по Classic ML, в первом нам требуется решить задачу классификации, а во втором задачу регрессии. В данных соревнованиях есть некоторые различия, однако все равно много общего, поэтому я составил для них единый минимальный список идей, которые предлагаю вам попробовать в контексте решения этих соревнования.

Я уже рассказывал, про общий алгоритм решения соревнований на Kaggle. Там, например, я упоминал, что основные идеи для своего решения можно найти в разделе с публичными ноутбуками и дискуссиями, однако, на случай если вы что-то пропустили, я перечислю основные вещи здесь. С этими концептами к данному моменту вы уже в той или мере знакомы, часть из них даже реализовывали вручную с нуля, теперь пришла пора применить и изучить их полноценно!

1. Exploration Data Analysis (EDA)
    
    1. Провести базовый анализ, посчитать статистики и распределения (matplotlib)
2. Предобработка
    
    1. Исследовать и обработать пропуски в данных, если таковые имеются
    2. Нормализовать данные, если потребуется
    3. Проверить наличие константных и скорелированных фичей
    4. Обработать категориальные фичи для моделей, которым это требуется, попробовать разные способы
    5. Изучить выбросы
    
    ** Сравнивайте метрику до и после*
    
3. Валидация
    
    1. Stratified K-fold
    
    ** Сравните скор на лидерборде у 1 и 5 фолдов*
    
4. Модели
    
    1. Классическая линейная регрессия (sklearn)
        
    2. Линейная регрессия с регуляризацией:
        
        1. Lasso
        2. Ridge
        3. ElasticNet
        
        ** Попробуйте разное значения параметра нормализации*
        
    3. KNN (sklearn) с разными гиперпараметрами (n_neighbors, weight, metric)
        
    4. Решающее дерево (sklearn)
        
    5. Random Forest (sklearn) с разными гиперпараметрами
        
    6. Бустинги
        
        1. CatBoost
        2. LightGBM
        3. XGBoost
        
        - _Обучить каждый и попробовать как можно больше параметров, описание которых можно найти в документации_
    7. Deep Neural Network (DNN)
        
        1. Начните с простой MLP из двух слоев и функции активации посредине
        2. Попробуйте добавить больше слоев
        3. Добавьте Batchnorm
        4. Попробуйте добавить Dropout с разными значениями
        5. Попробуйте разные размеры линейных слоев и разные функции активации
        6. Попробуйте разные оптимизаторы
        7. Попробуйте использовать scheduler (например, косинусовый)
        8. Попробуйте разные параметры для обучения (LR, batchsize, n_epochs, loss_fn)
        9. (Задание со звездочкой) попройте добавить Embedding слой для категориальных фичей
5. Feature Engineering
    
    1. Получите новые фичи путем декомпозиции текущих (например, времени и даты)
    2. Попробуйте комбинации фичей (например, наиболее высоких по feature importance)
    3. Попробуйте прочие трансформации и идеи для фичей
6. Ансамбли
    
    1. Усреднение
    2. Voting
    3. Stacking через линейную регрессию и Ridge
7. И не забудьте куда нибудь сложить все результаты и сравнить в конце